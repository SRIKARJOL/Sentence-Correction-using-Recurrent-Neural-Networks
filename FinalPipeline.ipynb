{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalPipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDHF45q8f-wO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Input,GRU\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "import pickle\n",
        "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM,TimeDistributed,Concatenate\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZxrV68ASA_b"
      },
      "source": [
        "# Encoder and Decoder with Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2EszE-8P-BE"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "\n",
        "        super().__init__()\n",
        "        self.inp_vocab_size = inp_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.input_length = input_length\n",
        "        self.lstm_size= lstm_size\n",
        "        self.lstm_output = 0\n",
        "        self.lstm_state_h=0\n",
        "        self.lstm_state_c=0\n",
        "\n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Encoder LSTM layer\n",
        "        self.embedding = Embedding(input_dim=self.inp_vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_decoder\")\n",
        "        self.lstm = LSTM(self.lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "\n",
        "    def call(self,input_sequence,states):\n",
        "      '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
        "      '''\n",
        "      input_embedd = self.embedding(input_sequence)\n",
        "      self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
        "      return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
        "    \n",
        "    def initialize_states(self,batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
        "      '''\n",
        "      return self.lstm_state_h,self.lstm_state_c\n",
        "      \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNHR65WL3_4r"
      },
      "source": [
        "import pdb\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "  '''\n",
        "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
        "  '''\n",
        "  def __init__(self,scoring_function, att_units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.scoring_function = scoring_function\n",
        "\n",
        "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
        "\n",
        "    if self.scoring_function=='dot':\n",
        "      # Intialize variables needed for Dot score function here\n",
        "      pass\n",
        "    if self.scoring_function == 'general':\n",
        "      # Intialize variables needed for General score function here\n",
        "      self.W = tf.keras.layers.Dense(att_units)\n",
        "    elif self.scoring_function == 'concat':\n",
        "      # Intialize variables needed for Concat score function here\n",
        "      self.W1 = tf.keras.layers.Dense(att_units)\n",
        "      self.W2 = tf.keras.layers.Dense(att_units)\n",
        "      self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  \n",
        "  def call(self,decoder_hidden_state,encoder_output):\n",
        "    '''\n",
        "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
        "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
        "        Multiply the score function with your encoder_outputs to get the context vector.\n",
        "        Function returns context vector and attention weights(softmax - scores)\n",
        "    '''\n",
        "    \n",
        "    decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
        "    if self.scoring_function == 'dot':\n",
        "        # Implement Dot score function here\n",
        "        score = tf.matmul(encoder_output , decoder_hidden_state,transpose_b=True)\n",
        "    elif self.scoring_function == 'general':\n",
        "        # Implement General score function here\n",
        "        #pdb.set_trace()\n",
        "        score = tf.matmul(self.W(encoder_output), decoder_hidden_state, transpose_b=True)\n",
        "    elif self.scoring_function == 'concat':\n",
        "        # Implement General score function here\n",
        "        score = self.V(tf.nn.tanh(self.W1(decoder_hidden_state) + self.W2(encoder_output)))\n",
        "\n",
        "    attention_weights = score\n",
        "    softmax_scores = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = softmax_scores * encoder_output    \n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COXf8eOv3_4t"
      },
      "source": [
        "class One_Step_Decoder(tf.keras.Model):\n",
        "      \n",
        "      def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "            super(One_Step_Decoder, self).__init__()\n",
        "            self.dec_units=dec_units\n",
        "            self.vocab_size = tar_vocab_size\n",
        "            self.embedding_dim = embedding_dim\n",
        "            self.input_length = input_length\n",
        "            self.attention = Attention(score_fun, dec_units)\n",
        "            self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, \n",
        "                                   name=\"embedding_layer_encoder\",weights=[decoder_embedding_matrix])\n",
        "\n",
        "            self.lstm = LSTM(self.dec_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "           # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "            self.DenseLayer = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "\n",
        "      def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
        "        \n",
        "        embedding= self.embedding(input_to_decoder)\n",
        "        context_vector,attention_weights = self.attention(state_h,encoder_output)\n",
        "        context_vector=tf.expand_dims(context_vector, 1)\n",
        "        lstm_input = tf.concat(\n",
        "                [tf.squeeze(context_vector, 1), tf.squeeze(embedding, 1)], 1)\n",
        "        states=[state_h,state_c]\n",
        "        lstm_input=tf.expand_dims(lstm_input, 1)\n",
        "        self.lstm_output, self.lstm_state_h,self.lstm_state_c= self.lstm(lstm_input, initial_state = states)\n",
        "        Output=self.DenseLayer(self.lstm_output)\n",
        "        Output=tf.reduce_sum(Output, axis=1)\n",
        "\n",
        "        return Output,self.lstm_state_h,self.lstm_state_c,attention_weights,context_vector"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5oSxAu3_4u"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "        \n",
        "        super(Decoder, self).__init__()\n",
        "        self.out_vocab_size=out_vocab_size\n",
        "        self.embedding_dim=embedding_dim\n",
        "        self.input_length=input_length\n",
        "        self.dec_units=dec_units\n",
        "        self.score_fun=score_fun\n",
        "        self.att_units=att_units\n",
        "\n",
        "        #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "        self.onestepdecoder=One_Step_Decoder(self.out_vocab_size, self.embedding_dim,  self.input_length, self.dec_units ,self.score_fun,self.att_units)\n",
        "  \n",
        "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
        "      \n",
        "        \n",
        "        all_outputs=tf.TensorArray(tf.float32,size=self.input_length,name=\"outputArray\")\n",
        "\n",
        "        for i in range(self.input_length):\n",
        "          decoder_input = tf.expand_dims(input_to_decoder[:, i], 1)\n",
        "          output,decoder_hidden_state,decoder_cell_state,attention_weights,context_vector=self.onestepdecoder(decoder_input,\n",
        "                                                                                                              encoder_output,\n",
        "                                                                                                              decoder_hidden_state,\n",
        "                                                                                                              decoder_cell_state)\n",
        "          \n",
        "          all_outputs=all_outputs.write(i,output)\n",
        "        \n",
        "        all_outputs=tf.transpose(all_outputs.stack(),[1,0,2])\n",
        "        \n",
        "        return all_outputs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQm5OYnyDAHN"
      },
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size,score_fun,attn_units):\n",
        "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
        "        self.score_fun=score_fun\n",
        "        self.attn_units=attn_units\n",
        "        self.encoder = Encoder(inp_vocab_size=vocab_size_sms+1, embedding_size=300,lstm_size=300, input_length=encoder_inputs_length)\n",
        "        self.decoder = Decoder(out_vocab_size=vocab_size_eng+1, embedding_dim=300, input_length=decoder_inputs_length,dec_units=300,score_fun=self.score_fun,att_units=self.attn_units)\n",
        "        \n",
        "               \n",
        "    def call(self, data):\n",
        "        input,output = data[0], data[1]\n",
        "    \n",
        "    \n",
        "        initial_state=self.encoder.initialize_states(50)\n",
        "        encoder_output, encoder_h, encoder_c = self.encoder(input,initial_state)\n",
        "        decoder_output                       = self.decoder(output,encoder_output, encoder_h, encoder_c)\n",
        "        return decoder_output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h39LYMfxTEcu"
      },
      "source": [
        "def lossfunction(y_true, y_pred): \n",
        "  crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True)\n",
        "    \n",
        "  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  loss = crossentropy(y_true, y_pred, sample_weight=mask)\n",
        "  \n",
        "  return loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toJ5HxFrTags"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def accuracy(y_true, y_pred): \n",
        "  pred_value= K.cast(K.argmax(y_pred, axis=-1), dtype='float32')\n",
        "  true_value = K.cast(K.equal(y_true, pred_value), dtype='float32')\n",
        "\n",
        "  mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "  n_correct = K.sum(mask * true_value)\n",
        "  n_total = K.sum(mask)\n",
        "  \n",
        "  return n_correct / n_total"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBEtvDUXpQr5",
        "outputId": "d73f0781-7790-4f19-b601-2e3c47e786dd"
      },
      "source": [
        "# Loading all the required information(embedding matrices and tokenziers)\n",
        "\n",
        "vocab_size_sms = 11613\n",
        "vocab_size_eng = 2800\n",
        "\n",
        "infile = open(\"/content/drive/MyDrive/CaseStudy2/decoder_embedding_matrix.pickle\",'rb')\n",
        "decoder_embedding_matrix = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open(\"/content/drive/MyDrive/CaseStudy2/encoder_embedding_matrix.pickle\",'rb')\n",
        "encoder_embedding_matrix = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open(\"/content/drive/MyDrive/CaseStudy2/sms_inp_test.pickle\",'rb')\n",
        "sms_inp_test = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open(\"/content/drive/MyDrive/CaseStudy2/eng_inp_test.pickle\",'rb')\n",
        "eng_inp_test = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open(\"/content/drive/MyDrive/CaseStudy2/eng_out_test.pickle\",'rb')\n",
        "eng_out_test = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "#pdb.set_trace()\n",
        "infile = open(\"/content/drive/MyDrive/CaseStudy2/sms_tokenizer.pickle\",'rb')\n",
        "sms_tokenizer = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "infile = open(\"/content/drive/MyDrive/CaseStudy2/eng_tokenizer.pickle\",'rb')\n",
        "eng_tokenizer = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "model  = encoder_decoder(encoder_inputs_length=51,decoder_inputs_length=42,output_vocab_size=vocab_size_eng+1,\n",
        "                         score_fun='concat',attn_units=50)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=lossfunction,metrics=[accuracy])\n",
        "\n",
        "model.fit([sms_inp_test, eng_inp_test], eng_out_test,\n",
        "          batch_size=50,\n",
        "          epochs=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 98s 389ms/step - loss: 2.4192 - accuracy: 0.0612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcba77f2490>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxoJ6YGUs0Ey"
      },
      "source": [
        "# https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
        "\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        " \n",
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "  sequences = [[list(), 0.0]]\n",
        "  for row in data:\n",
        "    all_candidates = list()\n",
        "    for i in range(len(sequences)):\n",
        "      seq, score = sequences[i]\n",
        "      for j in range(len(row)):\n",
        "        try:\n",
        "          candidate = [seq + [j], score - log(row[j])]\n",
        "          all_candidates.append(candidate)\n",
        "        except ValueError as e:\n",
        "          candidate = [seq + [j], 0]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "    ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "    # select k best\n",
        "    sequences = ordered[:k]\n",
        "  return sequences"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GXhwNaOpes6"
      },
      "source": [
        "import re\n",
        "def decontractions(phrase):\n",
        "  \"\"\"decontracted takes text and convert contractions into natural form.\n",
        "  https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
        "  # specific\n",
        "  phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "  phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "  phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
        "  phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
        "\n",
        "  # general\n",
        "  phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "  phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
        "  phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
        "  phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
        "  phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
        "  phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
        "  phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
        "  phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
        "  phrase = re.sub(r\"haha\",\" \",phrase)\n",
        "  phrase = re.sub(r\"hehe\",\" \",phrase)\n",
        "\n",
        "  return phrase\n",
        "\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = decontractions(text)\n",
        "  text = re.sub('[$)\\?\"’.°!;\\'€%:,(/]', '', text)\n",
        "  text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
        "  text = re.sub('[0-9]','',text)\n",
        "  return text  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGKQtA2-1_Dw"
      },
      "source": [
        "def load_model():\n",
        "  \"\"\"\n",
        "  return the model with loaded weights.\n",
        "  \"\"\"\n",
        "  model.load_weights(\"/content/drive/MyDrive/CaseStudy2/attention_model.h5\")\n",
        "\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZgxCynT3o4q"
      },
      "source": [
        "class ModelPrediction():\n",
        "  def __init__(self):\n",
        "    self.model_predict = load_model()\n",
        "\n",
        "  def predict(self,input_sentence):\n",
        "\n",
        "    max_length_targ = 42\n",
        "    max_length_inp  = 50\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess(input_sentence)\n",
        "    sentence = sentence.strip()\n",
        "    #pdb.set_trace()\n",
        "    inputs = [] \n",
        "    for word in sentence.split():\n",
        "        inputs.append(sms_tokenizer.word_index[word])\n",
        "    inputs =  tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp,padding='post') \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = '' \n",
        "\n",
        "    initial_state=self.model_predict.layers[0].initialize_states(batch_size=1)\n",
        "    encoder_outputs, state_h,state_c = self.model_predict.layers[0](inputs,initial_state)   \n",
        "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for i in range(max_length_targ):\n",
        "      Output,state_h,state_c,att_weights,_ = self.model_predict.layers[1].onestepdecoder(dec_input,encoder_outputs,\n",
        "                                                                                state_h,state_c,training=False)\n",
        "      #Beam Search Decoder\n",
        "      Result_beam_list=beam_search_decoder(Output,k=1)\n",
        "      Result_beam=Result_beam_list[0][0]\n",
        "\n",
        "      predicted_id = tf.argmax(Output[0]).numpy()\n",
        "    \n",
        "      result += eng_tokenizer.index_word[Result_beam[0]] + ' '\n",
        "      if eng_tokenizer.index_word[predicted_id] == '<end>':\n",
        "              return result\n",
        "\n",
        "      # the predicted ID is fed back into the model\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result   "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy70_4Mur6zc"
      },
      "source": [
        "model1 = ModelPrediction()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ba1hDSBGrKwM",
        "outputId": "efd07eeb-45d9-446e-b8e9-d7baea37d3d3"
      },
      "source": [
        "model1.predict(\"i juz said  also looking for someone to go\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i just said you also looking for someone to go <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IPWtO9zPquOJ",
        "outputId": "ba9b565f-11e4-4e6e-e816-e6bffab25631"
      },
      "source": [
        "model1.predict(\"haha ok one m going to make up late too\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ok i am going to get up late too <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yFmP7Hs2rIhf",
        "outputId": "4e612bd0-1303-405a-a39b-99442952aa60"
      },
      "source": [
        "model1.predict(\"okso did u get the idea i wuz green\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ok so did you get the idea i was green <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}